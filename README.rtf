{\rtf1\ansi\ansicpg936\cocoartf1561\cocoasubrtf400
{\fonttbl\f0\fswiss\fcharset0 ArialMT;\f1\fnil\fcharset134 PingFangSC-Regular;}
{\colortbl;\red255\green255\blue255;\red255\green0\blue0;}
{\*\expandedcolortbl;;\csgenericrgb\c100000\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab420
\pard\pardeftab420\ri720\qj\partightenfactor0

\f0\fs26 \cf0 The SupportDocs folder contain 3 folders: \'91code\'92, \'91caffemodel\'92 and \'91demo\'92.\
In the \'91code\'92 folder, I show the prototxt files (network structure), scripts when training and testing, scripts when applying in real scenes in life. \
In the \'91result\'92 folder, there are some trained model (caffemodel) with parameters and  saved predicted segmentation (.png). Due to limited size of uploaded file, I uploaded the required caffemodel to the Baidu web disk, which can be downloaded from here (Link: https://pan.baidu.com/s/1zsG-Mos7nzcg3kL2dF1sig  password: ajqj).  \
In the \'91demo\'92 folder, there are the original video and segmented demo video. As the same reason, please download it from https://pan.baidu.com/s/1NbxCDoiXTRO-sKe-bJZUAw (password: rf2m).\
\
The code is developed or applied under the following configurations.\
Hardware: 2-8 GPUs (with at least 12G GPU memories)\'a0\
Software: Ubuntu 16.04.3 LTS, CUDA 8.0, caffe, python, and OpenCV\
Dataset: ADE20K (download from {\field{\*\fldinst{HYPERLINK "http://groups.csail.mit.edu/vision/datasets/ADE20K/"}}{\fldrslt http://groups.csail.mit.edu/vision/datasets/ADE20K/}})\
\
When using my script on a new computer, the file_root of all scripts needs to be modified to your real file directory.\
\
\pard\pardeftab420\ri720\qj\partightenfactor0
\cf2 Train:\
\pard\pardeftab420\ri720\qj\partightenfactor0
\cf0 Before training, please prepare model structure (.prototxt), hyper parameter setting file (solver.prototxt) and model training script (train.sh). These files need to be changed based on the actual condition of the hardware.\
Makesure there has the ade_sceneparsing_train_im2cate
\f1 .txt file
\f0  with all image names for the training set. You can get it from the dataset classic web site.\
You also need the ImageNet-pre-trained model when fine-tuning. You can download the caffemodel from Baidu web disk too (link: https://pan.baidu.com/s/1CNCAEG4iwsXFUq0eoXsiyQ  password:8m8q).\
Enter the command in the terminal to train a model: sh train.sh\
\
\pard\pardeftab420\ri720\qj\partightenfactor0
\cf2 Test:\
\pard\pardeftab420\ri720\qj\partightenfactor0
\cf0 After successfully training out the network model, we need to evaluate the effectiveness of the trained model. Prepare the corresponding deploy.prototxt (network structure when testing) and validation.txt (names of test dataset).\
Enter the command in the terminal to evaluate the model on test set: python evaluate_seg.py\
It will produce a folder named \'91predict\'92, which contains the predicted segmentation result. The color.py can colorize the grayscale results.\
Enter the command in the terminal, you can see the score of mIOU, pixel accuracy and mean accuracy: python score.py\
\
\pard\pardeftab420\ri720\qj\partightenfactor0
\cf2 Apply on a scene image using our trained model:\
\pard\pardeftab420\ri720\qj\partightenfactor0
\cf0 Enter the command in the terminal: evaluate_demo.py\
If there is only an original video, you need to use save_frame.py to take pictures from the video. Use demolist.py to get the name list. Then use evaluate_demo.py to get predicted segmentation. Use color.py to visualize.\
}